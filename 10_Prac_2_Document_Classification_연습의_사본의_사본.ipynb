{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10 Prac 2. Document Classification 연습의 사본의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyuholee94/NLP/blob/master/10_Prac_2_Document_Classification_%EC%97%B0%EC%8A%B5%EC%9D%98_%EC%82%AC%EB%B3%B8%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecOHtGBs7lDO",
        "colab_type": "text"
      },
      "source": [
        "#10장 문서 분류 (Document Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSHRKud41Tr0",
        "colab_type": "text"
      },
      "source": [
        "#11-1 나이브 베이즈 분류(Naive Bayes Classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32FEohTU9qob",
        "colab_type": "text"
      },
      "source": [
        "##1.1 직접구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwzbT2LkrCwH",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvX-1FJ7qXOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = [['me free lottery', 1],\n",
        " ['free get free you', 1],\n",
        " ['you free scholarship', 0],\n",
        " ['free to contact me', 0],\n",
        " ['you won award', 0],\n",
        " ['you ticket lottery', 1]]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8irQ-xFsq39",
        "colab_type": "text"
      },
      "source": [
        "### 토큰 빈도수 및 문서별 토큰수 계산 (확률 계산을 위한 준비)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MLSe8V1sYsv",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://wikimedia.org/api/rest_v1/media/math/render/svg/98f086c560aa2f66650060277dda4f90e54e30c0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4KjiGtqsbm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "39c09f48-68e3-44ed-a4e9-0ee52c69c5ad"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 범주에 속하는 토큰수 세기 1(스팸), 0(정상))\n",
        "doccnt0 = 0\n",
        "doccnt1 = 0\n",
        "\n",
        "# 토큰별로 문서내 빈도수 카운팅\n",
        "wordfreq = defaultdict(lambda : [0, 0])\n",
        "\n",
        "for doc, label in training_set:\n",
        "  words = doc.split()\n",
        "  for word in words:\n",
        "    wordfreq[word][label] += 1\n",
        "\n",
        "for key, (cnt0, cnt1) in wordfreq.items():\n",
        "  doccnt0 += cnt0\n",
        "  doccnt1 += cnt1\n",
        "\n",
        "print('doccnt0 : {}'.format(doccnt0))\n",
        "print('doccnt1 : {}'.format(doccnt1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doccnt0 : 10\n",
            "doccnt1 : 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpySoJWVISKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7abd49e3-84e4-4fb5-dabb-3dc265334c68"
      },
      "source": [
        "doccnt0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOaDmUrzI0IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a48e9d7c-91e7-436e-c2dd-6197ca8abacb"
      },
      "source": [
        "doccnt1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKs_HaKwskLP",
        "colab_type": "text"
      },
      "source": [
        "### Training : 토큰별 조건부 확률 계산 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgCsmiCNsjjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "fb708fc0-d5d4-48b9-9baa-b60ea6d06bc4"
      },
      "source": [
        "k = 0.5\n",
        "\n",
        "wordprobs = defaultdict(lambda : [0, 0])\n",
        "for key, (cnt0, cnt1) in wordfreq.items():\n",
        "  wordprobs[key][0] = (cnt0 + k) /(2 * k + doccnt0)\n",
        "  wordprobs[key][1] = (cnt1 + k) /(2 * k + doccnt1)\n",
        "\n",
        "wordprobs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>,\n",
              "            {'award': [0.13636363636363635, 0.045454545454545456],\n",
              "             'contact': [0.13636363636363635, 0.045454545454545456],\n",
              "             'free': [0.22727272727272727, 0.3181818181818182],\n",
              "             'get': [0.045454545454545456, 0.13636363636363635],\n",
              "             'lottery': [0.045454545454545456, 0.22727272727272727],\n",
              "             'me': [0.13636363636363635, 0.13636363636363635],\n",
              "             'scholarship': [0.13636363636363635, 0.045454545454545456],\n",
              "             'ticket': [0.045454545454545456, 0.13636363636363635],\n",
              "             'to': [0.13636363636363635, 0.045454545454545456],\n",
              "             'won': [0.13636363636363635, 0.045454545454545456],\n",
              "             'you': [0.22727272727272727, 0.22727272727272727]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Viq9UFbs3Y2",
        "colab_type": "text"
      },
      "source": [
        "### Classify : 신규 텍스트가 주어졌을 때 확률 계산\n",
        "\n",
        ">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypuyUtI4LAQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d013a3b5-ba27-4e3e-cb37-f43952ef52f3"
      },
      "source": [
        "import math\n",
        "\n",
        "doc = \"free lottery\"\n",
        "tokens = doc.split()\n",
        "\n",
        "log_prob1 = log_prob0 = 0.0\n",
        "\n",
        "for word, (prob0, prob1) in wordprobs.items():\n",
        "  if word in tokens:\n",
        "    log_prob0 += math.log(prob0) \n",
        "    log_prob1 += math.log(prob1)\n",
        "\n",
        "log_prob0 += math.log(doccnt0/(doccnt0 + doccnt1))\n",
        "log_prob1 += math.log(doccnt1/(doccnt0 + doccnt1))\n",
        "\n",
        "prob0 = math.exp(log_prob0)\n",
        "prob1 = math.exp(log_prob1)\n",
        "\n",
        "print(prob0)\n",
        "print(prob1)\n",
        "\n",
        "print(\"정상확률 : {}\".format( prob0 /(prob0 + prob1) * 100))\n",
        "print(\"스팸확률 : {}\".format( prob1 /(prob0 + prob1) * 100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00516528925619835\n",
            "0.03615702479338842\n",
            "정상확률 : 12.500000000000009\n",
            "스팸확률 : 87.49999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUDYKAuAqKA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "d1b9d732-545a-4c9c-efc6-92b063585e74"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "print(twenty_train.target_names)\n",
        "print(twenty_train.data[0])\n",
        "print(twenty_train.target[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T64GKnkvrjaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "71aacbfd-ba9a-4ab5-af70-da0868c32776"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bfd5ccecde72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                      ('clf', MultinomialNB())])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtext_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'twenty_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Nitvm0sXjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MultinomialNB?"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNqEq-EosaPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "b8288570-de6b-4556-8cc2-187ff3487224"
      },
      "source": [
        "\n",
        "text_clf"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN_zv9ejtM_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "6ce75ce7-b313-4b5e-b2af-ead9fcfe76d6"
      },
      "source": [
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test',  shuffle=True)\n",
        "predicted = text_clf.predict(twenty_test.data)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-97a8cf8deea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtwenty_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtwenty_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_20newsgroups' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXcYkt_5t1lT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "4cdd975c-a4af-4092-8965-a0e3b818b1f6"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters_clf = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "                  'tfidf__use_idf': (True, False),\n",
        "                  'clf__alpha' : (1, 0.1, 0.01, 0.001, 0.0001)\n",
        "                  }\n",
        "\n",
        "gs_clf = GridSearchCV(text_clf, parameters_clf, n_jobs=-1, verbose=2)\n",
        "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
        "print('Best score : {}'.format(gs_clf.best_score_))\n",
        "best_parameters = gs_clf.best_estimator_.get_params()\n",
        "for param_name in sorted(list(best_parameters.keys())):\n",
        "  print(\"\\t{} : {}\".format(param_name, best_parameters[param_name]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-413b79786a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'twenty_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ4SrbLBwv6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "a961e899-ea17-41a6-9fd8-3feb51683f30"
      },
      "source": [
        "predicted = gs_clf.best_estimator_.predict(twenty_test.data)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a041555c45ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtwenty_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtvnAqwPmH3G",
        "colab_type": "text"
      },
      "source": [
        "1.3 sklearn 활용 (한글 뉴스 분류)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU6PXjNsyINu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "c11a8b7a-b397-4deb-8c0b-e3d9f2b2d70d"
      },
      "source": [
        "!wget https://github.com/kyungsoo-fininsight/mulcam_b/raw/master/data/2019news_1000.csv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-27 00:29:53--  https://github.com/kyungsoo-fininsight/mulcam_b/raw/master/data/2019news_1000.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kyungsoo-fininsight/mulcam_b/master/data/2019news_1000.csv [following]\n",
            "--2020-07-27 00:29:54--  https://raw.githubusercontent.com/kyungsoo-fininsight/mulcam_b/master/data/2019news_1000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11125708 (11M) [text/plain]\n",
            "Saving to: ‘2019news_1000.csv’\n",
            "\n",
            "2019news_1000.csv   100%[===================>]  10.61M  23.8MB/s    in 0.4s    \n",
            "\n",
            "2020-07-27 00:29:54 (23.8 MB/s) - ‘2019news_1000.csv’ saved [11125708/11125708]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7TO3wURmyY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "53cbebf6-8d09-42da-9089-d87b43eb68a4"
      },
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"./2019news_1000.csv\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 121 ms, sys: 16 ms, total: 137 ms\n",
            "Wall time: 146 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNMZq2JlnU_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVRFpCexn_L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXSPQ2AoNx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['category1'], random_state=0)\n",
        "\n",
        "text_clf = Pipeline(\n",
        "    [('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB(alpha=0.5))]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo4IvfqCpRZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = text_clf.fit(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sE9_VWJplIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0eb0aed-d154-4f62-d21c-86cec77e3e0b"
      },
      "source": [
        "import numpy as np\n",
        "predicted = text_clf.predict(X_test)\n",
        "np.mean(predicted == y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQKIYd04puac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "500c1689-89e9-45b2-f1bc-7535fee3b703"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameter_clf = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1,4)],\n",
        "                 'tfidf__use_idf': (True, False,),\n",
        "                 'clf__alpha':(1, 0.1, 0.01, 0.001, 0.0001, 0.00001)}\n",
        "\n",
        "gs_clf = GridSearchCV(text_clf, parameters_clf, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(X_train, y_train)\n",
        "best_parameters = gs_clf.best_estimator_.get_params()\n",
        "for param_name in sorted(list(best_parameters.keys())):\n",
        "  print('\\t{}:{}'.format(param_name, best_parameters[param_name]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tclf:MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\tclf__alpha:0.01\n",
            "\tclf__class_prior:None\n",
            "\tclf__fit_prior:True\n",
            "\tmemory:None\n",
            "\tsteps:[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))]\n",
            "\ttfidf:TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
            "\ttfidf__norm:l2\n",
            "\ttfidf__smooth_idf:True\n",
            "\ttfidf__sublinear_tf:False\n",
            "\ttfidf__use_idf:True\n",
            "\tvect:CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "\tvect__analyzer:word\n",
            "\tvect__binary:False\n",
            "\tvect__decode_error:strict\n",
            "\tvect__dtype:<class 'numpy.int64'>\n",
            "\tvect__encoding:utf-8\n",
            "\tvect__input:content\n",
            "\tvect__lowercase:True\n",
            "\tvect__max_df:1.0\n",
            "\tvect__max_features:None\n",
            "\tvect__min_df:1\n",
            "\tvect__ngram_range:(1, 1)\n",
            "\tvect__preprocessor:None\n",
            "\tvect__stop_words:None\n",
            "\tvect__strip_accents:None\n",
            "\tvect__token_pattern:(?u)\\b\\w\\w+\\b\n",
            "\tvect__tokenizer:None\n",
            "\tvect__vocabulary:None\n",
            "\tverbose:False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo66cT8irAly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}